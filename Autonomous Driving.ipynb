{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d01be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc16fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd917f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fc4c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1102..1382 -> 280-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\Pyton38\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-28.315412186380275\n",
      "Track generation: 956..1199 -> 243-tiles track\n",
      "Episode:2 Score:-17.355371900826448\n",
      "Track generation: 1194..1497 -> 303-tiles track\n",
      "Episode:3 Score:-33.77483443708654\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Episode:4 Score:-27.536231884058353\n",
      "Track generation: 1217..1526 -> 309-tiles track\n",
      "Episode:5 Score:-38.311688311688926\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41053e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe87e042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87481654, 0.64502937, 0.34901017], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9c90e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 83, 149,  67],\n",
       "        [187,  35, 205],\n",
       "        [ 91, 168,  57],\n",
       "        ...,\n",
       "        [150,  57,  18],\n",
       "        [200, 127, 156],\n",
       "        [115, 151, 146]],\n",
       "\n",
       "       [[114,  26,  44],\n",
       "        [ 91, 102,  89],\n",
       "        [ 18,   5, 181],\n",
       "        ...,\n",
       "        [218,  31, 128],\n",
       "        [181,  31, 195],\n",
       "        [232, 146,  85]],\n",
       "\n",
       "       [[ 26,  98, 225],\n",
       "        [193, 194,  99],\n",
       "        [101, 144,  41],\n",
       "        ...,\n",
       "        [  0, 130, 136],\n",
       "        [152,  51, 172],\n",
       "        [ 26,  85,  97]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 13, 141, 255],\n",
       "        [117, 105, 213],\n",
       "        [247, 197,  86],\n",
       "        ...,\n",
       "        [ 49, 168, 184],\n",
       "        [133,  70,  13],\n",
       "        [ 95,  99, 180]],\n",
       "\n",
       "       [[145,  79,   4],\n",
       "        [218,  70,  61],\n",
       "        [ 76, 233,  57],\n",
       "        ...,\n",
       "        [ 34, 255,   7],\n",
       "        [217,  41,  75],\n",
       "        [ 42,  89, 188]],\n",
       "\n",
       "       [[ 88,  54,  92],\n",
       "        [ 19,   0,  31],\n",
       "        [164,  13, 238],\n",
       "        ...,\n",
       "        [170,  87, 241],\n",
       "        [131,  11, 232],\n",
       "        [166, 219, 220]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac01da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5de17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a0ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1026..1293 -> 267-tiles track\n",
      "Logging to Training\\Logs\\PPO_8\n",
      "Track generation: 1171..1468 -> 297-tiles track\n",
      "Track generation: 1107..1388 -> 281-tiles track\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -50      |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x206658608b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06503f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a1fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9576f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f297391",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CarRacing-v0\"\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42e4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\Pyton38\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1160..1460 -> 300-tiles track\n",
      "Track generation: 1260..1579 -> 319-tiles track\n",
      "Track generation: 1095..1373 -> 278-tiles track\n",
      "Track generation: 960..1211 -> 251-tiles track\n",
      "Track generation: 1071..1343 -> 272-tiles track\n",
      "Track generation: 1244..1558 -> 314-tiles track\n",
      "Track generation: 1224..1534 -> 310-tiles track\n",
      "Track generation: 1023..1283 -> 260-tiles track\n",
      "Track generation: 1423..1783 -> 360-tiles track\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Track generation: 1251..1568 -> 317-tiles track\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-58.62806274443865, 5.511747140691266)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ef8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44e91401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Episode:1,Score:-49.09090909090984\n",
      "Track generation: 1269..1590 -> 321-tiles track\n",
      "Episode:2,Score:-62.500000000000846\n",
      "Track generation: 1182..1488 -> 306-tiles track\n",
      "Episode:3,Score:-57.37704918032868\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Episode:4,Score:-52.05479452054876\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "Episode:5,Score:-53.23741007194325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Testing our model\n",
    "episodes = 5 # test the environment 5 times\n",
    "for episodes in range(1,episodes+1): # looping through each episodes\n",
    "    obs = env.reset() # observation space\n",
    "    # Taking the obs and passing it through our model\n",
    "  # tells that which kind of the action is best for our work\n",
    "    done = False \n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        ob = np.copy(obs) \n",
    "        #print(type(obs))\n",
    "        #print(type(ob))\n",
    "        action, _ = model.predict(ob) # now using model here # returns model action and next state\n",
    "        # take that action to get the best reward\n",
    "        # for observation space we get the box environment\n",
    "        # rather than getting random action we are using model.predict(obs) on our obs for an curr env to gen the action inorder to get best possible reward\n",
    "        obs, reward, done, info = env.step(action)  # gies state, reward whose value is 1\n",
    "        # reward is 1 for every step including the termination step\n",
    "        score += reward\n",
    "    print('Episode:{},Score:{}'.format(episodes,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc87c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821760b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5869fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
